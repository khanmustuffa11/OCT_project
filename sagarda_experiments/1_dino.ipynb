{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec1047d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "import pandas as pd\n",
    "import PIL\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "588086c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(no):\n",
    "    torch.manual_seed(no)\n",
    "    random.seed(no)\n",
    "    np.random.seed(no)\n",
    "    os.environ['PYTHONHASHSEED'] = str()\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45cdbd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d454c740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "num_epochs = 20\n",
    "batchsize = 128\n",
    "lr = 0.0001\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 90\n",
    "LEARNING_RATE = 0.0001\n",
    "TRAIN_DATA_PATH = \"./Images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d50ddb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.ImageFolder(root=TRAIN_DATA_PATH, transform=data_transforms['train'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "875231fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                                           num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b2c2991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/sagarnildass/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "xFormers not available\n",
      "xFormers not available\n"
     ]
    }
   ],
   "source": [
    "dinov2_vits14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfeecbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DinoVisionTransformerClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DinoVisionTransformerClassifier, self).__init__()\n",
    "        self.transformer = dinov2_vits14\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(384, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 5)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.transformer(x)\n",
    "        x = self.transformer.norm(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f4b28a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf85c444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = DinoVisionTransformerClassifier()\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1ffd89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc275312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(y_true, y_pred):\n",
    "    \"\"\"Calculates accuracy between truth labels and predictions.\n",
    "\n",
    "    Args:\n",
    "        y_true (torch.Tensor): Truth labels for predictions.\n",
    "        y_pred (torch.Tensor): Predictions to be compared to predictions.\n",
    "\n",
    "    Returns:\n",
    "        [torch.float]: Accuracy value between y_true and y_pred, e.g. 78.45\n",
    "    \"\"\"\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a90ff26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] loss: 0.622\n",
      "[1,   100] loss: 0.652\n",
      "[1,   150] loss: 0.616\n",
      "[1,   200] loss: 0.637\n",
      "[1,   250] loss: 0.621\n",
      "Train accuracy: 75.70%\n",
      "[2,    50] loss: 0.624\n",
      "[2,   100] loss: 0.585\n",
      "[2,   150] loss: 0.603\n",
      "[2,   200] loss: 0.583\n",
      "[2,   250] loss: 0.588\n",
      "Train accuracy: 77.47%\n",
      "[3,    50] loss: 0.604\n",
      "[3,   100] loss: 0.573\n",
      "[3,   150] loss: 0.567\n",
      "[3,   200] loss: 0.558\n",
      "[3,   250] loss: 0.568\n",
      "Train accuracy: 77.87%\n",
      "[4,    50] loss: 0.552\n",
      "[4,   100] loss: 0.536\n",
      "[4,   150] loss: 0.556\n",
      "[4,   200] loss: 0.574\n",
      "[4,   250] loss: 0.530\n",
      "Train accuracy: 79.27%\n",
      "[5,    50] loss: 0.553\n",
      "[5,   100] loss: 0.540\n",
      "[5,   150] loss: 0.536\n",
      "[5,   200] loss: 0.529\n",
      "[5,   250] loss: 0.526\n",
      "Train accuracy: 79.53%\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs.to(device))\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        train_acc += accuracy_fn(y_true=labels.to(device),\n",
    "                                 y_pred=outputs.argmax(dim=1)) \n",
    "        if i % 50 == 49:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 50:.3f}')\n",
    "            running_loss = 0.0\n",
    "    train_acc /= len(train_loader)\n",
    "    print(f\"Train accuracy: {train_acc:.2f}%\")\n",
    "\n",
    "print('Finished Training')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b50195e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: models/x_ray_pretrain_dino_10_epochs.pth\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create models directory (if it doesn't already exist), see: https://docs.python.org/3/library/pathlib.html#pathlib.Path.mkdir\n",
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True, # create parent directories if needed\n",
    "                 exist_ok=True # if models directory already exists, don't error\n",
    ")\n",
    "\n",
    "# Create model save path\n",
    "MODEL_NAME = \"x_ray_pretrain_dino_10_epochs.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "# Save the model state dict\n",
    "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
    "torch.save(obj=model.state_dict(), # only saving the state_dict() only saves the learned parameters\n",
    "           f=MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6e8dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "artelus",
   "language": "python",
   "name": "artelus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
